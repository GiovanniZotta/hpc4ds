1.  Ogni processo legge la sua parte dal file di input
        Se le salva?

2.  Ogni processo calcola la mappa {item: support} di ognuno degli elementi delle sue transaction
        MPI_Allreduce con MPI_User_function che unisce due mappe {item: support}
            Risultato: ogni processo ha la mappa globale {item: support}

3.  Ogni processo, prende una parte delle chiavi della mappa (come fare?) e la ordina per support, mettendo il risultato in un sottovettore di chiavi ordinato
        MPI_Allreduce con MPI_User_function che unisce due array di stringhe ordinati
            Risultato: ogni processo ha l'array globale di chiavi ordinato

4.  Crea una mappa {item: indice nell'array ordinato} (ogni processo si fa la sua?)

5.  Ogni processo, per ogni transaction costruisce il suo sottoalbero e li unisce (openMP)
        MPI_Allreduce con MPI_User_function che unisce i due alberi
            Risultato: ogni processo ha l'albero globale

/**
 * 1. COMBINE HASHMAP
 *      - total_hashmap_length = allreduce(sum, local_hashmap_length)
 *      - resize local_hashmap (total_hashmap_length)
 *      - resize global_hashmap (total_hashmap_length) (HOW?? implement hashmap_resize) 
 *          ----- (CAN WE SKIP IF WE RESIZE INSIDE REDUCE? -> MAP IS GLOBAL)
 *      - define MPI datatype MPI_HM (size, totsize, entries(length: total_hashmap_length))
 *      - global_hashmap = Allreduce(global_hashmap, total_hashmap_length, MPI_HM)
 *      - free local_hash_map
 * 
 * 2. COMBINE FPTREE
 *      - How big should the tree be? how many nodes? how many child per node?
 *          - IDEA: avoid sending adjacence list, add optional field parent and send that,
 *                  in the user operation just rebuilt the adj list
 *          - for number of nodes: total_fp_size = sum??? -> allreduce(sum, local_fp_size)
 *      - resize local_fp_tree (total_fp_size)
 *      - resize global_fp_tree (total_fp_size)
 *      - MPI datatype MPI_FP: (item (char*), value (int), parent(int))
 *      - global_fp_tree = Allreduce (...)
 *      - free local_fp_tree
 * 
 * 
 * ALTERNATIVE:
 * 1. COMBINE FPTREE
 * - gather all FPTREEs (MPI_Allgatherv) & combine them with OpenMP
 *      (cons: each process does the same job)
 * 
 * 
 * CUSTOMREDUCE:
 * define MPI_Datatype HashmapEntry
 * - HASHMAP:
 *      SEND:   
 *              send array of localmap->data 
 * 
 *      RECEIVE: 
 *              MPI_probe
 *              get count and allocate map with sufficiently many entries
 *              receive hashmap_entries and store in receivemap->data
 *              count how many non void are there -> define a function
 *              return receivemap ?
 * 
 *     REDUCE:
 *      for i in range depth
 *          if I should send:
 *              SEND local hashmap
 *              globalmap = waitReduce
 *          else:
 *              receivemap = RECEIVE
 *              combine globalmap &= receivemap
 *      broadcast globalmap totalsize
 *      broadcast globalmap entries
 *
 *              
 *      
 */ 


TODO: Documentation of sort.c

-------------------------------------
BENCHMARKING

For each dataset, test each of the following configurations with support 0.05, 0.1, 0.2:

* 1 MPI process, 1 OMP thread
* 1 MPI process, 4 OMP threads
* 4 MPI processes, 1 OMP thread
* 4 MPI processes, 4 OMP threads
* 4 MPI processes, 8 OMP threads

Each test runs 5 times, so we can take an average of the running time.


MANIGOLDO: pct/out_2021-06-12-09-52-44_19_0.0001_32_8_0.6_static
